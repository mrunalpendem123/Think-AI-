# Think AI

**The first SLM-based search that works 100% locally in your browser.**

Think AI is a privacy-focused research assistant that runs Small Language Models (SLMs) directly on your device using WebGPU. It combines real-time web search with local AI processing to give you accurate, grounded answers without compromising your data.

## Features

-   **100% Local Inference:** Runs models like Qwen 2.5, Llama 3, and Gemma 2 entirely in your browser via WebLLM.
-   **Privacy-First:** Your questions and context never leave your machine.
-   **Deep Research:** Integrated with MiniRAG to perform intelligent, multi-step web searches and synthesis.
-   **Zero Latency:** No server queues or API costs for inference.

## Getting Started

1.  **Clone the repository:**
    ```bash
    git clone https://github.com/mrunalpendem123/Think-AI-.git
    cd Think-AI-
    ```

2.  **Install Frontend Dependencies:**
    ```bash
    cd frontend
    npm install
    ```

3.  **Run Locally:**
    ```bash
    npm run dev
    ```
    Open [http://localhost:3000](http://localhost:3000) to start thinking.
